{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook explains the modeling and training process\n",
    "\n",
    "### Table of content\n",
    "\n",
    "** I. Import Neccessary libraries <br>\n",
    "II. Import Dataset <br>\n",
    "III. Data Cleansing and processing <br>\n",
    "IV. Initializing the neural network <br>\n",
    "V. Model Training **\n",
    "\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## I. Import Neccessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### If missing any library, please uncomment the repective line below and pip install\n",
    "#!pip install tensorflow --upgrade\n",
    "#!pip install h5py\n",
    "#!pip install numpy --upgrade\n",
    "#!pip install pandas\n",
    "#!pip install dask --upgrade\n",
    "\n",
    "## Taken from https://pypi.org/project/pygeohash/\n",
    "## Using this instead of the python-geohash by hiwi due to better documentation\n",
    "\n",
    "#!pip install pygeohash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Coddy\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import pygeohash as pgh\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "tqdm_notebook.pandas()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Import the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Spilting the training dataset (sorely for github)\n",
    "Had an issue where github do not allow commit above 100MB.<br>\n",
    "My laptop had several issues in regarding the usage of github-lfs for large file<br>\n",
    "Thus I spilt them up into two 80~ MB files, and combine them later in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Load and spilt the dataframe\n",
    "try:\n",
    "    df = pd.read_csv(\"training.csv\")\n",
    "    df1 = df[:int(df.shape[0]/2)]\n",
    "    df2 = df[int(df.shape[0]/2):]\n",
    "\n",
    "## Save to csv\n",
    "    df1.to_csv(\"training_1st_half.csv\", index_label = False)\n",
    "    df2.to_csv(\"training_2nd_half.csv\", index_label = False)\n",
    "\n",
    "## Already uploaded the sub dataset, this should go to the pass path all the time.\n",
    "except:\n",
    "    pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"training_1st_half.csv\")\n",
    "df2 = pd.read_csv(\"training_2nd_half.csv\")\n",
    "df = df1.append(df2, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An brief overview of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 4206321 rows and 4 columns\n",
      "A brief summary of numerical columns:\n",
      "                day        demand\n",
      "count  4.206321e+06  4.206321e+06\n",
      "mean   3.145299e+01  1.050907e-01\n",
      "std    1.768278e+01  1.592655e-01\n",
      "min    1.000000e+00  3.092217e-09\n",
      "25%    1.600000e+01  1.867379e-02\n",
      "50%    3.200000e+01  5.043463e-02\n",
      "75%    4.700000e+01  1.208644e-01\n",
      "max    6.100000e+01  1.000000e+00\n",
      "First five rows of the dataset:\n",
      "  geohash6  day timestamp    demand\n",
      "0   qp03wc   18      20:0  0.020072\n",
      "1   qp03pn   10     14:30  0.024721\n",
      "2   qp09sw    9      6:15  0.102821\n",
      "3   qp0991   32       5:0  0.088755\n",
      "4   qp090q   15       4:0  0.074468\n"
     ]
    }
   ],
   "source": [
    "print(\"The dataset has \" + str(df.shape[0]) + \" rows and \" + str(df.shape[1]) + \" columns\")  # 4,206,321 entries\n",
    "print(\"A brief summary of numerical columns:\")\n",
    "print(df.describe())\n",
    "print(\"First five rows of the dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Data cleansing and processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Combine day and timestamp columns to time_stamp column\n",
    "This column measure the index of timestamp from day 1 first time stamp (0) to day 61 last timestamp (5855)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68cda8ca38514542a42c2daed9e56357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geohash6</th>\n",
       "      <th>day</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>demand</th>\n",
       "      <th>time_stamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qp03wc</td>\n",
       "      <td>18</td>\n",
       "      <td>20:0</td>\n",
       "      <td>0.020072</td>\n",
       "      <td>1712.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qp03pn</td>\n",
       "      <td>10</td>\n",
       "      <td>14:30</td>\n",
       "      <td>0.024721</td>\n",
       "      <td>922.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qp09sw</td>\n",
       "      <td>9</td>\n",
       "      <td>6:15</td>\n",
       "      <td>0.102821</td>\n",
       "      <td>793.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qp0991</td>\n",
       "      <td>32</td>\n",
       "      <td>5:0</td>\n",
       "      <td>0.088755</td>\n",
       "      <td>2996.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qp090q</td>\n",
       "      <td>15</td>\n",
       "      <td>4:0</td>\n",
       "      <td>0.074468</td>\n",
       "      <td>1360.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  geohash6  day timestamp    demand  time_stamp\n",
       "0   qp03wc   18      20:0  0.020072      1712.0\n",
       "1   qp03pn   10     14:30  0.024721       922.0\n",
       "2   qp09sw    9      6:15  0.102821       793.0\n",
       "3   qp0991   32       5:0  0.088755      2996.0\n",
       "4   qp090q   15       4:0  0.074468      1360.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def string_to_time (string):\n",
    "    x = string.split(\":\")\n",
    "    timing = int(x[0]) * 60 + int(x[1])\n",
    "    return timing/15\n",
    "\n",
    "df['time_stamp'] = df['timestamp'].progress_apply(string_to_time)\n",
    "df['time_stamp'] = df['time_stamp'] + (df['day'] - 1)*96\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>demand</th>\n",
       "      <th>time_stamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.206321e+06</td>\n",
       "      <td>4.206321e+06</td>\n",
       "      <td>4.206321e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.145299e+01</td>\n",
       "      <td>1.050907e-01</td>\n",
       "      <td>2.964249e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.768278e+01</td>\n",
       "      <td>1.592655e-01</td>\n",
       "      <td>1.697748e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.092217e-09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.600000e+01</td>\n",
       "      <td>1.867379e-02</td>\n",
       "      <td>1.477000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.200000e+01</td>\n",
       "      <td>5.043463e-02</td>\n",
       "      <td>3.010000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.700000e+01</td>\n",
       "      <td>1.208644e-01</td>\n",
       "      <td>4.428000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.100000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>5.855000e+03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                day        demand    time_stamp\n",
       "count  4.206321e+06  4.206321e+06  4.206321e+06\n",
       "mean   3.145299e+01  1.050907e-01  2.964249e+03\n",
       "std    1.768278e+01  1.592655e-01  1.697748e+03\n",
       "min    1.000000e+00  3.092217e-09  0.000000e+00\n",
       "25%    1.600000e+01  1.867379e-02  1.477000e+03\n",
       "50%    3.200000e+01  5.043463e-02  3.010000e+03\n",
       "75%    4.700000e+01  1.208644e-01  4.428000e+03\n",
       "max    6.100000e+01  1.000000e+00  5.855000e+03"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()   ##Time stamp from 0 to 5855 (5856 = 61 x24 x 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Identify unique geohash6-s\n",
    "A total of 1329 unique geohash markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1329\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['qp02yc', 'qp02yf', 'qp02yu', ..., 'qp0dnh', 'qp0dnj', 'qp0dnn'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de = df.groupby(['geohash6']).count()\n",
    "hash_list = de.index.values\n",
    "hash_list = np.squeeze(hash_list)\n",
    "print(len(hash_list))\n",
    "hash_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Pivot the original 4206321 by 4 dataframe to time_stamp vs geohash6 (5847 by 1329)\n",
    "There's 5856 - 5847 = 9 timestamps where no demand is observed in any geohash<br>\n",
    "The dataframe is then filled with 0 to replace NaN to prevent any numerical calculation errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5847, 1329)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>geohash6</th>\n",
       "      <th>qp02yc</th>\n",
       "      <th>qp02yf</th>\n",
       "      <th>qp02yu</th>\n",
       "      <th>qp02yv</th>\n",
       "      <th>qp02yy</th>\n",
       "      <th>qp02yz</th>\n",
       "      <th>qp02z1</th>\n",
       "      <th>qp02z3</th>\n",
       "      <th>qp02z4</th>\n",
       "      <th>qp02z5</th>\n",
       "      <th>...</th>\n",
       "      <th>qp0djv</th>\n",
       "      <th>qp0djw</th>\n",
       "      <th>qp0djy</th>\n",
       "      <th>qp0dn0</th>\n",
       "      <th>qp0dn1</th>\n",
       "      <th>qp0dn4</th>\n",
       "      <th>qp0dn5</th>\n",
       "      <th>qp0dnh</th>\n",
       "      <th>qp0dnj</th>\n",
       "      <th>qp0dnn</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_stamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004056</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009381</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008253</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002701</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1329 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "geohash6    qp02yc  qp02yf  qp02yu  qp02yv  qp02yy  qp02yz  qp02z1  qp02z3  \\\n",
       "time_stamp                                                                   \n",
       "0.0            0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1.0            0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2.0            0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3.0            0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4.0            0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "geohash6    qp02z4  qp02z5  ...  qp0djv  qp0djw    qp0djy  qp0dn0  qp0dn1  \\\n",
       "time_stamp                  ...                                             \n",
       "0.0            0.0  0.0000  ...     0.0     0.0  0.000000     0.0     0.0   \n",
       "1.0            0.0  0.0006  ...     0.0     0.0  0.004056     0.0     0.0   \n",
       "2.0            0.0  0.0000  ...     0.0     0.0  0.000000     0.0     0.0   \n",
       "3.0            0.0  0.0000  ...     0.0     0.0  0.000000     0.0     0.0   \n",
       "4.0            0.0  0.0000  ...     0.0     0.0  0.008253     0.0     0.0   \n",
       "\n",
       "geohash6    qp0dn4  qp0dn5  qp0dnh    qp0dnj  qp0dnn  \n",
       "time_stamp                                            \n",
       "0.0            0.0     0.0     0.0  0.000000     0.0  \n",
       "1.0            0.0     0.0     0.0  0.000000     0.0  \n",
       "2.0            0.0     0.0     0.0  0.009381     0.0  \n",
       "3.0            0.0     0.0     0.0  0.000000     0.0  \n",
       "4.0            0.0     0.0     0.0  0.002701     0.0  \n",
       "\n",
       "[5 rows x 1329 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.pivot_table(df, values='demand', index=['time_stamp'],columns=['geohash6'])\n",
    "df = df.fillna(0)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Converting the time_stamp vs geohash6 table to a list of historical data for each demand entry to be the input of model\n",
    "This is the most difficult part of the data processing section.<br><br>\n",
    "I would like to sample past 13 days record for each entry, if applicable( to be safe from exceeding limit of up to 14days, also to reduce dataframe size to a small extent)<br>\n",
    "However this will add additonal 13 x 24 x 4 -1 = 1247 columns to the existing dataset, rendering it impossible to be handled on local server due to memory issues in processing a ~ 4millon by 1250 dataframe. (roughly 180 GB of RAM for such df)<br><br>\n",
    "To tackle this, I adopt the minibatch technique and use a random sampling method to sample 20 geohash using a rolling window technique on the training dataset.<br> This provides me with roughly 50,000 entries for each sampling frame as training set. <br><br>\n",
    "By keeping the parameters from previous learning cycles and resampling training set, this repeated process will achieve an approximate accuracy of training on the entire dataset (minibatch idea)<br>\n",
    "The development set is not resampled, as it is a very small.<br><br>\n",
    "**Training and Development set format**:<br>\n",
    "**Features consisting of**:<br>\n",
    "1248 columns of demand from timestamp = T-1248 to timestamp = T-1<br>\n",
    "1 column of the demand at timestamp = T<br>\n",
    "2 columns of geohash6 data in terms of latitude and longitude. This is decoded using pygeohash library <br>\n",
    "**Targets being**:<br>\n",
    "5 columns the demand at timestamp = T+1 to timestamp = T+5 <br>\n",
    "**This makes a total of 1251 features and 5 targets **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resampling(df,seed = 10):\n",
    "    '''\n",
    "    This function takes in 3 arguments: a training dataframe, a limit (int) and a seed (int) for randomization\n",
    "    \n",
    "    This function randomly pick 30 geohash6 from the list of available geohash6 in the training dataframe based on the seed\n",
    "    It then perform a rolling window of 1248 + 1 + 5 = 1254 width on the dataframe's subset of only the selected geohash6-s\n",
    "    It then append the result of the rolling window as well as the decoded geohash to the X and Y dataframe respectively\n",
    "    It repeats until the X (or Y, same length) exceed length of limit argument or when all 30 geohash6 are completed.\n",
    "    \n",
    "    This function returns the training X and Y of the randomed sample\n",
    "    '''\n",
    "    X = pd.DataFrame({\"index\":list(np.core.defchararray.add(\"T-\",np.arange(1248,0,-1).astype(\"str\")))+\\\n",
    "                           [\"T\",\"lat\",\"lon\"]})\n",
    "    Y = pd.DataFrame({\"index\":[\"T+1\",\"T+2\",\"T+3\",\"T+4\",\"T+5\"]})\n",
    "    \n",
    "    hash_list = df.T.index.values\n",
    "    \n",
    "    random.seed(seed)\n",
    "    try:\n",
    "        sample = random.sample(range(0, len(hash_list)), 20)\n",
    "    except:\n",
    "        sample = hash_list\n",
    "    k=0\n",
    "    sampled_list = []\n",
    "    for geohash in tqdm_notebook(hash_list[sample]):\n",
    "        lat = pgh.decode(geohash)[0]\n",
    "        lon = pgh.decode(geohash)[1]\n",
    "        for i in range(1248,df.shape[0]-5):\n",
    "            if df[geohash].values[i] > 0:\n",
    "                k+=1\n",
    "                X[str(k)] = list(df[geohash].values[i-1248:i+1])+[lat,lon]\n",
    "                Y[str(k)] = list(df[geohash].values[i+1:i+6])\n",
    "        sampled_list += [geohash]\n",
    "        \n",
    "    print(len(sampled_list),\"Geohash-s sampled,\",len(hash_list)-len(sampled_list),\" remaining\")\n",
    "\n",
    "    X=np.array(X.T.drop(['index'])).astype(\"float\")\n",
    "    Y=np.array(Y.T.drop(['index'])).astype(\"float\")\n",
    "    \n",
    "    print(\"This sample has:\",k,\"rows\")\n",
    "    \n",
    "    return X,Y ,sampled_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dev_sampling(df):\n",
    "    '''\n",
    "    This function takes in 1 argument: a dataframe\n",
    "     \n",
    "    This function performs a rolling window of 1248 + 1 + 5 = 1254 width on the dataframe\n",
    "    It then append the result of the rolling window as well as the decoded geohash to the X and Y dataframe respectively\n",
    "    It repeats until it loops through the entire dataframe\n",
    "    \n",
    "    This function returns the dev X and Y of the randomed sample\n",
    "    '''\n",
    "    X = pd.DataFrame({\"index\":list(np.core.defchararray.add(\"T-\",np.arange(1248,0,-1).astype(\"str\")))+\\\n",
    "                           [\"T\",\"lat\",\"lon\"]})\n",
    "    Y = pd.DataFrame({\"index\":[\"T+1\",\"T+2\",\"T+3\",\"T+4\",\"T+5\"]})\n",
    "    \n",
    "    hash_list = df.T.index.values\n",
    "    \n",
    "    k=0\n",
    "\n",
    "    for geohash in tqdm_notebook(hash_list):\n",
    "        lat = pgh.decode(geohash)[0]\n",
    "        lon = pgh.decode(geohash)[1]\n",
    "        for i in range(1248,df.shape[0]-5):\n",
    "            if df[geohash].values[i] > 0:\n",
    "                try:\n",
    "                    k+=1\n",
    "                    X[str(k)] = list(df[geohash].values[i-1248:i+1])+[lat,lon]\n",
    "                    Y[str(k)] = list(df[geohash].values[i+1:i+6])\n",
    "                except:\n",
    "                    k+=1\n",
    "                    print(df[geohash].values[i])\n",
    "                    print(list(df[geohash].values[i-1248:i+6])+[geohash])\n",
    "                    \n",
    "    X=np.array(X.T.drop(['index'])).astype(\"float\")\n",
    "    Y=np.array(Y.T.drop(['index'])).astype(\"float\")\n",
    "    \n",
    "    print(\"This sample has:\",k,\"rows\")\n",
    "    \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I used a train testt split of 99% training and 1% testing.<br>\n",
    "This gives me 14 geohashs of data in the developement set **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train,dev = train_test_split(df.T, test_size=0.01, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A mock sampling of training set** (used to debug the initiation of neural network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_X_0, train_Y_0,_ = resampling(train.T,99)\n",
    "# print(\"Training set have the shape of:\",(train_X_0.shape,train_Y_0.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sampling of the development set** <br>\n",
    "Loading from existing set (seed = 42), if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc34549e5b624e6fb1824e4b360ea773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    dev_X,dev_Y = pickle.load(open(\"dev.pkl\",'rb'))\n",
    "    print(\"Loaded Development set have the shape of:\",(dev_X.shape,dev_Y.shape))\n",
    "except:\n",
    "    dev_X,dev_Y = dev_sampling(dev.T)\n",
    "    print(\"Reloaded Development set have the shape of:\",(dev_X.shape,dev_Y.shape))\n",
    "    filehandler = open(\"dev.pkl\",\"wb\")\n",
    "    pickle.dump((dev_X,dev_Y),filehandler)\n",
    "    filehandler.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Initializing the Neural Network Model\n",
    "\n",
    "### 1. Preparation\n",
    "\n",
    "**A 3 layers neural network with tensorflow framework was used for this challenge.**<br>\n",
    "<br>\n",
    "The layers are of following:<br>\n",
    "L-0: Input layer of 1251 features <br>\n",
    "L-1: A hidden layer of 250 features <br>\n",
    "L-2: A hidden layer of 50 features <br>\n",
    "L-3: A predicting layer of 5 features<br><br>\n",
    "<br>\n",
    "The pipline is Linear - Relu - Linear - Relu - Linear <br>\n",
    "Cost is measured by mean square error, same as required in the challenge<br>\n",
    "<br>\n",
    "Due to memory issue mentioned above, I train each training sample set for 30 epoches before resampling.<br>\n",
    "This is repeated for \n",
    "\n",
    "__*I would like to thank and credit deeplearning.ai for providing me with a basic template for Neural Network and a few useful functions via their courses. *__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders(n_x,n_y):\n",
    "    '''\n",
    "    This function creates Tensorflow placeholders of correct dimensions\n",
    "    '''\n",
    "    X = tf.placeholder(tf.float32, [None,n_x], name=\"X\")\n",
    "    Y = tf.placeholder(tf.float32, [None,n_y], name=\"Y\")\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For Testing and Debugging\n",
    "X, Y = create_placeholders(120, 5)\n",
    "print(\"X = \" + str(X))\n",
    "print(\"Y = \" + str(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x,n_y,parameters = None):\n",
    "    '''\n",
    "    This function initialized the 3 layers neural network with the correct dimensions.\n",
    "    Randoming the variables in the first time\n",
    "    Assigning the past parameter if given --> allowing transfer learning\n",
    "    '''\n",
    "    \n",
    "    if parameters != None:\n",
    "        \n",
    "        W1 = tf.get_variable(\"W1\", initializer = parameters['W1'])\n",
    "        b1 = tf.get_variable(\"b1\", initializer = parameters['b1'])\n",
    "        W2 = tf.get_variable(\"W2\", initializer = parameters['W2'])\n",
    "        b2 = tf.get_variable(\"b2\", initializer = parameters['b2'])\n",
    "        W3 = tf.get_variable(\"W3\", initializer = parameters['W3'])\n",
    "        b3 = tf.get_variable(\"b3\", initializer = parameters['b3'])\n",
    "        print(\"Past parameters updated\")\n",
    "    \n",
    "    else:\n",
    "        W1 = tf.get_variable(\"W1\", [n_x, 250], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        b1 = tf.get_variable(\"b1\", [1],initializer = tf.zeros_initializer())\n",
    "        W2 = tf.get_variable(\"W2\", [250, 50], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        b2 = tf.get_variable(\"b2\", [1], initializer = tf.zeros_initializer())\n",
    "        W3 = tf.get_variable(\"W3\", [50, n_y], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        b3 = tf.get_variable(\"b3\", [1], initializer = tf.zeros_initializer())\n",
    "        \n",
    "    param = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2,\n",
    "              \"W3\": W3,\n",
    "              \"b3\": b3}\n",
    "    \n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For Testing and Debugging \n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    test_parameters = initialize_parameters(1251,5)\n",
    "    print(\"W1 = \" + str(test_parameters[\"W1\"]))\n",
    "    print(\"b1 = \" + str(test_parameters[\"b1\"]))\n",
    "    print(\"W2 = \" + str(test_parameters[\"W2\"]))\n",
    "    print(\"b2 = \" + str(test_parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    '''\n",
    "    This function obtain coefficient of various parameters and use them to predict a final cost(Z3)\n",
    "    This process consists of  a linear function of X @ W1 + b1, @ being matrix multiplication,\n",
    "        followed by a retilinear activation function \n",
    "    '''\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    #print(X)\n",
    "    Z1 = tf.add(tf.matmul(X,W1), b1)                      \n",
    "    A1 = tf.nn.relu(Z1)  \n",
    "    Z2 = tf.add(tf.matmul(A1,W2), b2)     \n",
    "    A2 = tf.nn.relu(Z2)    \n",
    "    Z3 = tf.add(tf.matmul(A2,W3), b3)  \n",
    "\n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For Testing and Debugging \n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(1251, 5)\n",
    "    parameters = initialize_parameters(1251,5)\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    print(\"Z3 = \" + str(Z3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(Z3, Y,parameters):\n",
    "    '''\n",
    "    This function compute the mean square error bewteen the 5 entries in predicted Z3 and the 5 entries in actual Y\n",
    "    '''\n",
    "    #print(Z3,Y)\n",
    "    m = Y.shape[1] \n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]    \n",
    "    W3 = parameters[\"W3\"]\n",
    "    \n",
    "    rmse_cost = tf.reduce_mean(tf.losses.mean_squared_error(predictions=Z3, labels=Y))\n",
    "    \n",
    "    # Loss function using L2 Regularization\n",
    "    regularizer = tf.nn.l2_loss(W1) + tf.nn.l2_loss(W2) + tf.nn.l2_loss(W3)\n",
    "\n",
    "    \n",
    "    # add cross_entropy_cost and L2_regularization_cost\n",
    "    cost = tf.reduce_mean(rmse_cost + 0.01 * regularizer)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For Testing and Debugging \n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(1251, 5)\n",
    "    parameters = initialize_parameters(1251,5)\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    cost = compute_cost(Z3, Y,parameters)\n",
    "    print(\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The below function **random_mini_batches() **is obtained from deeplearning.ai on 9th June 2019. unedited**<br>\n",
    "It is used to create random mini batches by shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[ permutation,:]\n",
    "    shuffled_Y = Y[permutation,:].reshape(m,(Y.shape[1]))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n",
    "        mini_batch_Y = shuffled_Y[ k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[ num_complete_minibatches * mini_batch_size : m,:]\n",
    "        mini_batch_Y = shuffled_Y[ num_complete_minibatches * mini_batch_size : m,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model Training and Transfer Learning\n",
    "The below function initiates the tensorflow session and provide the possibility for transfer learning on previously trained parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(train,dev_X,dev_Y,iterations = 10,learning_rate = 0.0001, k = 0,\n",
    "          num_epochs = 30, batch_size = 64, print_cost = True,parameters = None,train_acc =None, dev_acc = None):\n",
    "    '''\n",
    "    Initiate the Tensorflow session and combine all previous functions.\n",
    "    Using an AdamOptizer to minimise the mean square error of the prediction\n",
    "    Allow transfer learning using previous parameters\n",
    "    return trained parameter, train and dev acc for plotting\n",
    "    '''\n",
    "    ops.reset_default_graph()\n",
    "    \n",
    "    r = 1\n",
    "    tf.set_random_seed(r)\n",
    "    seed = random.randint(1,1000)\n",
    "    \n",
    "    #Initial sampling of training data\n",
    "    # Try to load exisiting data (fast, ~5sec), else resample from train dataset (slow ~ 10min)\n",
    "    try:\n",
    "        train_X,train_Y,sample_list = pickle.load(open(\"./temp_training_set/train\"+str(k)+\".pkl\",'rb'))\n",
    "        print(\"loaded exising dataset\")\n",
    "    except:\n",
    "        print(\"Resampling new dataset\")\n",
    "        train_X, train_Y, sample_list = resampling(train)  \n",
    "        filehandler = open(\"./temp_training_set/train\"+str(k)+\".pkl\",\"wb\")\n",
    "        pickle.dump((train_X,train_Y,sample_list),filehandler)\n",
    "        filehandler.close()\n",
    "        \n",
    "    (m,n_x) = train_X.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = train_Y.shape[1]                            # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    if dev_acc == None:\n",
    "        dev_acc = []                                          # To keep track of dev accuracy\n",
    "    if train_acc == None:\n",
    "        train_acc = []                                     # To keep track of train accuracy\n",
    "    \n",
    "    ## Setting various pipeline functions\n",
    "    X, Y = create_placeholders(n_x, n_y)              #Create placeholder\n",
    "\n",
    "    param = initialize_parameters(n_x,n_y,parameters)     #Randomise the first set of parameters with the correct dimension\n",
    "\n",
    "    Z3 = forward_propagation(X, param)           #Calculate Z3 using lin-relu-lin-relu-lin\n",
    "    \n",
    "    cost = compute_cost(Z3, Y,param)                        #Compute the cost of this iteration\n",
    "   \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Use an Adam Optimatizer for the cost\n",
    "    \n",
    "    init = tf.global_variables_initializer()        \n",
    "\n",
    "    #Start the tensorflow Session\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Looping for each resampling session\n",
    "        for i in range(iterations):\n",
    "            print(\"Iteration\",k)\n",
    "            k += 1\n",
    "            for epoch in tqdm_notebook(range(num_epochs)):\n",
    "\n",
    "                epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "                num_minibatches = int(m / batch_size)\n",
    "\n",
    "                seed = seed + 1\n",
    "                minibatches = random_mini_batches(train_X,train_Y, batch_size, seed)\n",
    "                \n",
    "                \n",
    "                # looping through minibatches\n",
    "                for minibatch in minibatches:\n",
    "\n",
    "                    # Select a minibatch\n",
    "                    (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "                    _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "\n",
    "                    epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "\n",
    "                # Print the cost every 10 epoches in the resampling frame\n",
    "                if print_cost == True and epoch % 10 == 0:\n",
    "                    print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "                if print_cost == True and epoch % 5 == 0:\n",
    "                    costs.append(epoch_cost)\n",
    "                    \n",
    "            \n",
    "            # Caculate the error            \n",
    "            rmse = tf.reduce_mean(tf.losses.mean_squared_error(predictions=Z3, labels=Y))\n",
    "            dev_acc.append(rmse.eval({X:dev_X, Y: dev_Y}))\n",
    "            train_acc.append(rmse.eval({X:train_X, Y: train_Y}))\n",
    "            print(\"Training error: {:.6f}\".format(rmse.eval({X:train_X, Y: train_Y})))\n",
    "            print(\"Validation error: {:.6f}\".format(rmse.eval({X:dev_X, Y: dev_Y})))\n",
    "            \n",
    "            \n",
    "            #Drop the trained column and resample the training set after finishing the last epoch\n",
    "            train = train.drop(sample_list,axis=1)  \n",
    "            \n",
    "            if i < iterations-1:\n",
    "                # Try to load exisiting data (fast, ~5sec), else resample from train dataset (slow ~ 10min)\n",
    "                try:\n",
    "                    train_X,train_Y,sample_list = pickle.load(open(\"./temp_training_set/train\"+str(k)+\".pkl\",'rb'))\n",
    "                    print(\"loaded exising dataset\")\n",
    "                except:\n",
    "                    print(\"Resampling new dataset\")\n",
    "                    train_X, train_Y, sample_list = resampling(train)  \n",
    "                    filehandler = open(\"./temp_training_set/train\"+str(k)+\".pkl\",\"wb\")\n",
    "                    pickle.dump((train_X,train_Y,sample_list),filehandler)\n",
    "                    filehandler.close()\n",
    "  \n",
    "\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        #plt.ylim(0, 0.002) \n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate) + \"\\nResampling Iterations =\" + str(iterations))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(param)\n",
    "        print(\"Parameters have been trained!\")\n",
    "        \n",
    "        \n",
    "        # save pkl\n",
    "        print(\"Saving parameters\")\n",
    "        filehandler = open(\"training_parameters.pkl\",\"wb\")\n",
    "        pickle.dump((parameters,train_acc,dev_acc,train,k),filehandler)\n",
    "        filehandler.close()\n",
    "        print(\"pkl saved\")\n",
    "        \n",
    "        # return variable to continue next sampling frame\n",
    "        return parameters, train_acc,dev_acc,train,k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V Model Training\n",
    "### 1. Training processing\n",
    "#### Loading past trained data\n",
    "Try to load any past trained data using pickle<br>\n",
    "If no past trained data is found, initiate a new neural network and train from scratch\n",
    "#### Calling the function model()\n",
    "Calling without the 'parameters' argument initiate the neural network via randoming weights<br>\n",
    "Calling with the 'parameters' argument being past trained parameters allow the network to continue tuning on previously trained parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initial training\n",
    "\n",
    "## Uncomment the below line to start the first cycle of training\n",
    "\n",
    "# parameters,train_acc,dev_acc = model(train,dev_X,dev_Y,iterations= 1)\n",
    "\n",
    "### Transfer learning\n",
    "## Loading or Retrain\n",
    "try:\n",
    "    parameters,train_acc,dev_acc,train,k  = pickle.load(open(\"training_parameters.pkl\",'rb'))\n",
    "    print(\"Parameters loaded\")\n",
    "except:\n",
    "    print(\"Restarting training from the beginning\")\n",
    "    parameters,train_acc,dev_acc,train,k = model(train.T,dev_X,dev_Y,iterations= 1)\n",
    "    \n",
    "## Training\n",
    "parameters,train_acc,dev_acc,train,k = model(train,dev_X,dev_Y,iterations= 64,\n",
    "                                                    parameters = parameters,k=k,\n",
    "                                                    train_acc=train_acc,dev_acc=dev_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Plotting Results\n",
    "The below graph shows the mean sqaure error on development set and training set vs number of iterations of training & sampling.<br>\n",
    "Note the the error on development set may fluctuate rapidly due to the random sampling of training set.<br>\n",
    "This fluctuate will/should decrease as the number of iterations increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(dev_acc)\n",
    "plt.plot(train_acc)\n",
    "plt.ylabel('Mean square error')\n",
    "plt.legend([\"Development\",\"Training\"])\n",
    "plt.title(\"Overall and Zoomed in plot\")\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(dev_acc)\n",
    "plt.plot(train_acc)\n",
    "plt.xlabel('Resampling Iterations')\n",
    "plt.ylabel('Mean square error')\n",
    "plt.legend([\"Development\",\"Training\"])\n",
    "plt.ylim(0,0.02)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
